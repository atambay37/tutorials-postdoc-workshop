{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# OLMO RAG DEMO\n",
    "\n",
    "Now it's your turn to apply your data and specific domain knowledge.\n",
    "\n",
    "You can use this notebook as a starting point and adapt it to your needs.\n",
    "You will need to develop the pre-processing stage for a RAG system.\n",
    "This includes document retrieval, cleaning, chunking,\n",
    "and ingestion into the vector database using an embedding model.\n",
    "\n",
    "To help you, we've provided a few example code snippets in Jupyter notebooks found in the \n",
    "[`appendix`](../appendix/index.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6d3aa35-0685-4d8a-9424-28f54a207ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from testcontainers.qdrant import QdrantContainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b628a08b-b9c4-4759-9954-732b543dc51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant = QdrantContainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f01106fa-2eea-4d65-ab3c-24a4cfdf2564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pulling image testcontainers/ryuk:0.8.1\n",
      "Container started: ec5e308cc66e\n",
      "Waiting for container <Container: ec5e308cc66e> with image testcontainers/ryuk:0.8.1 to be ready ...\n",
      "Pulling image qdrant/qdrant:v1.8.3\n",
      "Container started: dab23a1f4954\n",
      "Waiting for container <Container: dab23a1f4954> with image qdrant/qdrant:v1.8.3 to be ready ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<testcontainers.qdrant.QdrantContainer at 0x1164b7b50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc3d0a74-2e0c-4c77-8464-2169de64731a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for container <Container: dab23a1f4954> with image qdrant/qdrant:v1.8.3 to be ready ...\n",
      "Waiting for container <Container: dab23a1f4954> with image qdrant/qdrant:v1.8.3 to be ready ...\n"
     ]
    }
   ],
   "source": [
    "client = qdrant.get_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b624653f",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "A section for whatever utility functions you need. We have packaged up our utility functions in a Python package called `ssec_tutorials`. You can find the source code in this [GitHub repository](https://github.com/uw-ssec/ssec_tutorials)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc327339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here for whatever utility functions you need. This can be anything such as\n",
    "# cleaning up document format, setting up prompt templates, etc.\n",
    "\n",
    "\n",
    "# Uncomment the following for a simple document formatting function\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4105a812",
   "metadata": {},
   "source": [
    "## Retrieve documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cf9ba5",
   "metadata": {},
   "source": [
    "A section for document retrieval. This just means getting your document from whatever sources,\n",
    "in your local computer or the internet. See the [Document Loaders](https://python.langchain.com/v0.2/docs/integrations/document_loaders/) integration list from Langchain for an extensive list of what's possible.\n",
    "\n",
    "For the purpose of this tutorial, we recommend a simple example of loading a piece of text from a file such as PDF. Also, if you have a large piece of text, you can split it into smaller chunks using Langchains's [RecursiveTextSplitter](https://python.langchain.com/v0.2/docs/how_to/recursive_text_splitter/).\n",
    "\n",
    "If you don't have any data with you, you can try out with this [Algorithm Textbook by Jeff Erickson](http://jeffe.cs.illinois.edu/teaching/algorithms/book/Algorithms-JeffE.pdf). This textbook has been generously made available by Jeff Erickson under the [Creative Commons Attribution 4.0 International license](http://creativecommons.org/licenses/by/4.0/), you can find more information about the textbook at [https://jeffe.cs.illinois.edu/teaching/algorithms/](https://jeffe.cs.illinois.edu/teaching/algorithms/).\n",
    "\n",
    "```{note}\n",
    "If you're running things on Codespace, [refer to this link](https://stackoverflow.com/questions/62284623/how-can-i-upload-a-file-to-a-github-codespaces-environment) and upload your data to `resources/` folder. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6994afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here for your retrieval step,\n",
    "# see the documentation on PyMuPDF for more information:\n",
    "# https://python.langchain.com/v0.2/docs/how_to/document_loader_pdf/#using-pymupdf\n",
    "\n",
    "# Uncomment below for code to download the textbook\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "url = \"http://jeffe.cs.illinois.edu/teaching/algorithms/book/Algorithms-JeffE.pdf\"\n",
    "filename = os.path.basename(url)\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    # Download if file doesn't exist\n",
    "    pdf_path, headers = urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b37fedc-7003-47f0-9753-58a668922e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "pdf_folder_path = \".\"  # update path to point to the relevant directory\n",
    "documents = []\n",
    "for file in os.listdir(pdf_folder_path):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(pdf_folder_path, file)\n",
    "        loader = PyMuPDFLoader(pdf_path)\n",
    "        documents.extend(loader.load())\n",
    "\n",
    "# for each in documents:\n",
    "#     # print(each.page_content) # Uncomment this line to see the individual page_content\n",
    "#     print(each.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434737ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here to load the PDF document as a Langchain Document objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eca40cd",
   "metadata": {},
   "source": [
    "## Document Embeddings to Qdrant Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c53d1",
   "metadata": {},
   "source": [
    "Once you've figured out how to retrieve and load your documents to Langchain Document objects, you can then proceed to loading these documents to Qdrant Vector Database collection.\n",
    "\n",
    "See the following documentation for some guidance on [Langchain Qdrant integration](https://python.langchain.com/v0.2/docs/integrations/vectorstores/qdrant/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35899c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d199a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lsetiawan/Repos/SSEC/tutorials/demos/olmo-rag/.pixi/envs/default/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/lsetiawan/Repos/SSEC/tutorials/demos/olmo-rag/.pixi/envs/default/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Setup the embedding, we are using the MiniLM model here\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657c623",
   "metadata": {},
   "source": [
    "### Setup Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b4ec3337-446a-4078-a78d-513ea301114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import models\n",
    "from langchain_qdrant import Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14d7fd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here to load your data into the database\n",
    "\n",
    "# uncomment below to set the Qdrant path and collection name\n",
    "# for an \"local mode\" on-disk storage\n",
    "# See https://python.langchain.com/v0.2/docs/integrations/vectorstores/qdrant/#on-disk-storage\n",
    "# qdrant_path = \"./my_qdrant_database\"\n",
    "qdrant_collection = \"algorithms_book\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4f040608-8c4d-4030-8c43-8b0fc6679a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating collection: algorithms_book\n"
     ]
    }
   ],
   "source": [
    "if not client.collection_exists(qdrant_collection):\n",
    "    print(\"Creating collection:\", qdrant_collection)\n",
    "    client.create_collection(\n",
    "        qdrant_collection,\n",
    "        vectors_config=models.VectorParams(\n",
    "            size=embedding.client.get_sentence_embedding_dimension(),\n",
    "            distance=models.Distance.COSINE,\n",
    "        ),\n",
    "    )\n",
    "    lcqdrant = Qdrant(\n",
    "        client=client, collection_name=qdrant_collection, embeddings=embedding\n",
    "    )\n",
    "    uuids = lcqdrant.add_documents(documents=documents)\n",
    "else:\n",
    "    lcqdrant = Qdrant(\n",
    "        client=client, collection_name=qdrant_collection, embeddings=embedding\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82a6fd7",
   "metadata": {},
   "source": [
    "### Test out the Qdrant collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8a6de6",
   "metadata": {},
   "source": [
    "At this step, you should have a Qdrant object (`langchain_qdrant.vectorstores.Qdrant`) that has your document loaded into it in a collection. You can test out the collection by querying for a documents and checking if the results are as expected.\n",
    "\n",
    "To do this, you'll need to create a [`VectorStoreRetriever`](https://python.langchain.com/v0.2/docs/how_to/vectorstore_retriever/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82691eec",
   "metadata": {},
   "source": [
    "```{note}\n",
    "A sample question example to ask from the document can be `\"What is the most familiar method for multiplying large numbers?\"`.\n",
    "An answer to this question can be found on page 3, section 0.2 Multiplication, Lattice Multiplication.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b934d230",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "You'll probably need to tweak the arguments for creating a `VectorStoreRetriever` object for the best search type and limiting the number of documents. This part is a bit of trial and error, so don't be afraid to experiment. It is a critical part of RAG system to get the right documents for the question as that is what the LLM would use to generate the answer.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ebbd18af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here to try out the vector database retrieval with a question query\n",
    "retriever = lcqdrant.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "624c8fdd-f11a-44ad-8974-bd2e0b67383a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'author': '', 'creationDate': \"D:20190613170427-05'00'\", 'creator': 'LaTeX with hyperref', 'file_path': './Algorithms-JeffE.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': \"D:20190613170427-05'00'\", 'page': 20, 'producer': 'pdfTeX-1.40.19', 'source': './Algorithms-JeffE.pdf', 'subject': '', 'title': '', 'total_pages': 472, 'trapped': '', '_id': 'ac1baae8-80c1-421e-b283-5e17150e1337', '_collection_name': 'algorithms_book'}, page_content='0.2. Multiplication\\nto mechanical techniques for place-value arithmetic using “Arabic” numerals.\\nPeople trained in the fast and reliable execution of these procedures were called\\nalgorists or computators, or more simply, computers.\\n0.2\\nMultiplication\\nAlthough they have been a topic of formal academic study for only a few decades,\\nalgorithms have been with us since the dawn of civilization. Descriptions of\\nstep-by-step arithmetic computation are among the earliest examples of written\\nhuman language, long predating the expositions by Fibonacci and al-Khw¯arizm¯ı,\\nor even the place-value notation they popularized.\\nLattice Multiplication\\nThe most familiar method for multiplying large numbers, at least for American\\nstudents, is the lattice algorithm. This algorithm was popularized by Fibonacci\\nin Liber Abaci, who learned it from Arabic sources including al-Khw¯arizm¯ı, who\\nin turn learned it from Indian sources including Brahmagupta’s 7th-century\\ntreatise Br¯ahmasphut.asiddh¯anta, who may have learned it from Chinese sources.\\nThe oldest surviving descriptions of the algorithm appear in The Mathematical\\nClassic of Sunzi, written in China between the 3rd and 5th centuries, and in\\nEutocius of Ascalon’s commentaries on Archimedes’ Measurement of the Circle,\\nwritten around 500ce, but there is evidence that the algorithm was known much\\nearlier. Eutocius credits the method to a lost treatise of Apollonius of Perga,\\nwho lived around 300bce, entitled Okytokion (᾿Ωκυτόκιον).7 The Sumerians\\nrecorded multiplication tables on clay tablets as early as 2600bce, suggesting\\nthat they may have used the lattice algorithm.8\\nThe lattice algorithm assumes that the input numbers are represented as\\nexplicit strings of digits; I’ll assume here that we’re working in base ten, but the\\nalgorithm generalizes immediately to any other base. To simplify notation,9 the\\n7Literally “medicine that promotes quick and easy childbirth”! Pappus of Alexandria repro-\\nduced several excerpts of Okytokion about 200 years before Eutocius, but his description of the\\nlattice multiplication algorithm (if he gave one) is also lost.\\n8There is ample evidence that ancient Sumerians calculated accurately with extremely\\nlarge numbers using their base-60 place-value numerical system, but I am not aware of any\\nsurviving record of the actual methods they used.\\nIn addition to standard multiplication\\nand reciprocal tables, tables listing the squares of integers from 1 to 59 have been found,\\nleading some math historians to conjecture that Babylonians multiplied using an identity like\\nx y = ((x + y)2 −x2 −y2)/2. But this trick only works when x + y < 60; history is silent on how\\nthe Babylonians might have computed x2 when x ≥60.\\n9but at the risk of inﬂaming the historical enmity between Greece and Egypt, or Lilliput and\\nBlefuscu, or Macs and PCs, or people who think zero is a natural number and people who are\\nwrong\\n3\\n'),\n",
       " Document(metadata={'author': '', 'creationDate': \"D:20190613170427-05'00'\", 'creator': 'LaTeX with hyperref', 'file_path': './Algorithms-JeffE.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': \"D:20190613170427-05'00'\", 'page': 38, 'producer': 'pdfTeX-1.40.19', 'source': './Algorithms-JeffE.pdf', 'subject': '', 'title': '', 'total_pages': 472, 'trapped': '', '_id': 'bc28cc39-f88b-47bb-b661-4c6fffa5edfe', '_collection_name': 'algorithms_book'}, page_content='The control of a large force is the same principle as the control of a few men:\\nit is merely a question of dividing up their numbers.\\n— Sun Zi, The Art of War (c. 400CE), translated by Lionel Giles (1910)\\nOur life is frittered away by detail.... Simplify, simplify.\\n— Henry David Thoreau, Walden (1854)\\nNow, don’t ask me what Voom is. I never will know.\\nBut, boy! Let me tell you, it DOES clean up snow!\\n— Dr. Seuss [Theodor Seuss Geisel], The Cat in the Hat Comes Back (1958)\\nDo the hard jobs ﬁrst. The easy jobs will take care of themselves.\\n— attributed to Dale Carnegie\\n1\\nRecursion\\n1.1\\nReductions\\nReduction is the single most common technique used in designing algorithms.\\nReducing one problem X to another problem Y means to write an algorithm\\nfor X that uses an algorithm for Y as a black box or subroutine. Crucially, the\\ncorrectness of the resulting algorithm for X cannot depend in any way on how\\nthe algorithm for Y works. The only thing we can assume is that the black box\\nsolves Y correctly. The inner workings of the black box are simply none of our\\nbusiness; they’re somebody else’s problem. It’s often best to literally think of the\\nblack box as functioning purely by magic.\\nFor example, the peasant multiplication algorithm described in the previous\\nchapter reduces the problem of multiplying two arbitrary positive integers to\\nthree simpler problems: addition, mediation (halving), and parity-checking. The\\nalgorithm relies on an abstract “positive integer” data type that supports those\\nthree operations, but the correctness of the multiplication algorithm does not\\n21\\n')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is the most familiar method for multiplying large numbers?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Setup OLMo Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb84bc",
   "metadata": {},
   "source": [
    "At this stage now we have the Retrieval-Augmented (RA) in RAG system. Let's now setup the Generation (G) part with the OLMo model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3b202679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssec_tutorials import download_olmo_model\n",
    "\n",
    "# This will download the OLMO model to the cache directory\n",
    "OLMO_MODEL = download_olmo_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5bba3081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this line to understand your available options for LlamaCpp Class\n",
    "# LlamaCpp?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "# Here we've setup the LlamaCpp model,\n",
    "# but you'll need to add additional arguments to `LlamaCpp`\n",
    "# to make it work for your specific use case\n",
    "olmo = LlamaCpp(\n",
    "    model_path=str(OLMO_MODEL),\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    verbose=False,\n",
    "    n_ctx=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e3181c",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "Try asking some questions to OLMo about any content of the document you've loaded in the Qdrant collection.\n",
    "You will find that the OLMo model is not trained on your specific domain, so it might not give you the best results.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "39dccb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The one most people still use, despite being taught otherwise.\n",
      "The most familiar method for multiplying large numbers is the long hand method. In this method, you write out the multiplication problem as a story, using words to represent each digit and then writing the equal sign once all the words are in place. For example: 2 3 4 5 6 7 8 * 9 = ? 4800 The answer would be 4800.\n",
      "The most familiar method for multiplying large numbers is still used today because it's simple, easy to understand, and requires little mental arithmetic. It also allows you to visualize the problem and make sure you have everything in the right order before attempting to write an equal sign.\n",
      "What is the traditional way of writing down the product of two or more numbers?\n",
      "The traditional way of writing down the product of two or more numbers is to use a comma-separated list of numbers, with each number followed by five dots (..) and then another set of numbers separated by commas. For example: 2, 3, 4, 5, 6 The correct way to write this would be \"2, 3, 4, 5, 6\" or \"2000, 3000, 4000, 5000, 6000\". This traditional method is still used today for large numbers because"
     ]
    }
   ],
   "source": [
    "_ = olmo.invoke(input=\"What is the most familiar method for multiplying large numbers?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e8e5a9",
   "metadata": {},
   "source": [
    "Rather than a just a simple question, we'll need to refine the prompt to include instruction and context for the model to generate the answer. To do this, we'll need to setup the proper string [PromptTemplate](https://python.langchain.com/v0.2/docs/concepts/#string-prompttemplates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Create the initial prompt template using OLMo's tokenizer chat template we saw in module 1.\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=olmo.client.metadata[\"tokenizer.chat_template\"],\n",
    "    template_format=\"jinja2\",\n",
    "    partial_variables={\"add_generation_prompt\": True, \"eos_token\": \"<|endoftext|>\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd751a28",
   "metadata": {},
   "source": [
    "Set the question for the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3d1b5a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the most familiar method for multiplying large numbers?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef522c06",
   "metadata": {},
   "source": [
    "Set the context for the prompt.\n",
    "This is where you'll need to use the `VectorStoreRetriever` and format the document object with `format_docs`\n",
    "or simply add your own text to the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757265b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment variable below to set the context\n",
    "# context = <Enter code or string here>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9569a7",
   "metadata": {},
   "source": [
    "Set the instruction for the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06338896",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"You are a computer science professor.\n",
    "Please answer the following question based on the given context.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d3c7b",
   "metadata": {},
   "source": [
    "The original OLMo chat template takes in multiple messages with a `role` and `content` key. You can use this template to ask questions to the model. For simplicity, we'll just use a single message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592dd248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below to set the input text template\n",
    "# input_text_template = f\"\"\"\\\n",
    "# {instruction}\n",
    "\n",
    "# Context: {context}\n",
    "\n",
    "# Question: {question}\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c540731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below to set the message dictionary\n",
    "# message = {\n",
    "#     \"role\": \"user\",\n",
    "#     \"content\": input_text_template,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below to try out the prompt template\n",
    "# print(prompt_template.format(\n",
    "#     messages=[message]\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c4fcab",
   "metadata": {},
   "source": [
    "You can see above what the final prompt looks like. There are tags like `<|user|>` that signify the model that this is a user input and so on. This final string is sent to the model for generating the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4584fdb",
   "metadata": {},
   "source": [
    "At this point you have all the parts for RAG system setup. Now let's chain the prompt engineering, OLMo model and the Qdrant collection to get a more accurate answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ee065902-46d2-4262-90bb-331fef284dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most familiar method for multiplying large numbers is the lattice algorithm. This algorithm was popularized by Fibonacci in \"Liber Abaci\" and is still widely used today, especially in American education. This method reduces multiplication to addition and subtraction, making it more accessible and efficient for larger numbers. The lattice algorithm has its roots in ancient civilizations such as China, India, and Egypt, where they used the technique of lattice division or \"lattice method,\" based on place-value notation."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The most familiar method for multiplying large numbers is the lattice algorithm. This algorithm was popularized by Fibonacci in \"Liber Abaci\" and is still widely used today, especially in American education. This method reduces multiplication to addition and subtraction, making it more accessible and efficient for larger numbers. The lattice algorithm has its roots in ancient civilizations such as China, India, and Egypt, where they used the technique of lattice division or \"lattice method,\" based on place-value notation.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Set the question\n",
    "question = \"What is the most familiar method for multiplying large numbers?\"\n",
    "\n",
    "# 2. Set the context\n",
    "context = format_docs(retriever.invoke(question))\n",
    "\n",
    "# 3. Set the instruction\n",
    "instruction = \"\"\"You are a computer science professor.\n",
    "Please answer the following question based on the given context.\"\"\"\n",
    "\n",
    "# 4. Set the input text template\n",
    "input_text_template = f\"\"\"\\\n",
    "{instruction}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# 5. Set the message dictionary\n",
    "message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": input_text_template,\n",
    "}\n",
    "\n",
    "# 6. Chain the prompt template and olmo model\n",
    "llm_chain = prompt_template | olmo\n",
    "\n",
    "# 7. Invoke the chain\n",
    "llm_chain.invoke(input={\"messages\": [message]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819c05b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here to create the retrieval chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7f3978",
   "metadata": {},
   "source": [
    "```{admonition} Answer Example Code\n",
    ":class: hint dropdown\n",
    "\n",
    "```{code-block} python\n",
    "# 1. Set the question\n",
    "question = \"What is the most familiar method for multiplying large numbers?\"\n",
    "\n",
    "# 2. Set the context\n",
    "context = format_docs(retriever.invoke(question))\n",
    "\n",
    "# 3. Set the instruction\n",
    "instruction = \"\"\"You are a computer science professor.\n",
    "Please answer the following question based on the given context.\"\"\"\n",
    "\n",
    "# 4. Set the input text template\n",
    "input_text_template = f\"\"\"\\\n",
    "{instruction}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# 5. Set the message dictionary\n",
    "message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": input_text_template,\n",
    "}\n",
    "\n",
    "# 6. Chain the prompt template and olmo model\n",
    "llm_chain = prompt_template | olmo\n",
    "\n",
    "# 7. Invoke the chain\n",
    "llm_chain.invoke(input={\"messages\": [message]})\n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "16f6b25f-aae4-4507-9d6b-f84bd2d8e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7df7cd",
   "metadata": {},
   "source": [
    "**Bonus: Try to create a simple chat app, by modifying the [1-olmo-chat-rag.ipynb](./1-olmo-chat-rag.ipynb) notebook with your use case.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69a5e52",
   "metadata": {},
   "source": [
    "Please fill out the [survey feedback form](https://tinyurl.com/ssecfeedback) to help us improve the tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pixi - Python 3 (ipykernel)",
   "language": "python",
   "name": "pixi-kernel-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
